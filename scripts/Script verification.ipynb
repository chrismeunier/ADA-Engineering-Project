{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|  textID|   topicDistribution|\n",
      "+--------+--------------------+\n",
      "|  671240|[6.58155862613258...|\n",
      "| 3781244|[9.40955320799835...|\n",
      "|  761241|[0.00146956887930...|\n",
      "| 1921241|[7.87280666829929...|\n",
      "| 2241240|[0.00141175238583...|\n",
      "| 4001241|[6.90012310329306...|\n",
      "|14091243|[7.49673752511427...|\n",
      "|  821241|[0.99567634446045...|\n",
      "|  441241|[0.24788906994921...|\n",
      "| 1171241|[4.98378122588521...|\n",
      "| 2251241|[3.46593827223224...|\n",
      "| 4451244|[7.20856067245151...|\n",
      "|  711243|[0.19224600963891...|\n",
      "| 2221244|[5.73554304172097...|\n",
      "|  311241|[3.87982960614948...|\n",
      "| 1191240|[3.00254584416535...|\n",
      "| 1041241|[0.14659772641234...|\n",
      "|13961244|[0.99420762787174...|\n",
      "|14231242|[0.21207572174111...|\n",
      "|14761242|[1.90451862605562...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_lda = spark.read.parquet('result_lda.parquet')\n",
    "result_lda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_rdd = sc.textFile('../sample_data/now-samples-sources.txt').zipWithIndex().filter(lambda r: r[1] > 2).keys().map(lambda r: r.split('\\t'))\n",
    "\n",
    "#create schema and change data type for date\n",
    "sources_schema = sources_rdd.map(lambda r: Row(textID=int(r[0]),nwords=int(r[1]),date=r[2],country=r[3],website=r[4],url=r[5],title=r[6],))\n",
    "sources = spark.createDataFrame(sources_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf to change list of topic distribution into multiple columns\n",
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "\n",
    "distribution = (result_lda.withColumn(\"topic\", to_array(col(\"topicDistribution\"))).select([\"textID\"] + [col(\"topic\")[i] for i in range(10)]))\n",
    "country = sources.drop(\"date\",\"nwords\",\"title\",\"url\",\"website\")\n",
    "country = country.select(col(\"country\"), col(\"textID\").alias(\"c_textID\"))\n",
    "country_dist = distribution.join(country, distribution.textID == country.c_textID).drop(\"c_textID\")\n",
    "avg_countryTopics = country_dist.sort(\"textID\").groupby(\"country\").mean().drop(\"avg(textID)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldColumns = avg_countryTopics.schema.names[-10:]\n",
    "newColumns = ['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(oldColumns)):\n",
    "    avg_countryTopics = avg_countryTopics.withColumnRenamed(oldColumns[i], newColumns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year(date): integer (nullable = true)\n",
      " |-- month(date): integer (nullable = true)\n",
      " |-- avg(topic[0]): double (nullable = true)\n",
      " |-- avg(topic[1]): double (nullable = true)\n",
      " |-- avg(topic[2]): double (nullable = true)\n",
      " |-- avg(topic[3]): double (nullable = true)\n",
      " |-- avg(topic[4]): double (nullable = true)\n",
      " |-- avg(topic[5]): double (nullable = true)\n",
      " |-- avg(topic[6]): double (nullable = true)\n",
      " |-- avg(topic[7]): double (nullable = true)\n",
      " |-- avg(topic[8]): double (nullable = true)\n",
      " |-- avg(topic[9]): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_dateTopics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sources.drop(\"country\",\"nwords\",\"title\",\"url\",\"website\")\n",
    "dates = dates.select(col(\"date\"), col(\"textID\").alias(\"d_textID\"))\n",
    "date_dist = distribution.join(dates, distribution.textID == dates.d_textID).drop(\"d_textID\")\n",
    "avg_dateTopics = date_dist.sort(\"textID\").groupBy(year(\"date\"),month(\"date\")).mean().drop('avg(textID)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------------+-----------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|year(date)|month(date)|      avg(topic[0])|    avg(topic[1])|      avg(topic[2])|      avg(topic[3])|      avg(topic[4])|      avg(topic[5])|      avg(topic[6])|      avg(topic[7])|    avg(topic[8])|      avg(topic[9])|\n",
      "+----------+-----------+-------------------+-----------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|      null|       null|0.07705842133774261|0.104873906575239|0.11313226969777704|0.05024047759621304|0.13021743878510075|0.15288021279302708|0.09880137527229328|0.07381243261961451|0.082581498775322|0.11640196654767081|\n",
      "+----------+-----------+-------------------+-----------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_dateTopics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+-----------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|year|month|             topic0|           topic1|             topic2|             topic3|             topic4|             topic5|             topic6|             topic7|           topic8|             topic9|\n",
      "+----+-----+-------------------+-----------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|null| null|0.07705842133774261|0.104873906575239|0.11313226969777704|0.05024047759621304|0.13021743878510075|0.15288021279302708|0.09880137527229328|0.07381243261961451|0.082581498775322|0.11640196654767081|\n",
      "+----+-----+-------------------+-----------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('avg_dateTopics.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(k=10, maxIter=10, optimizeDocConcentration=True).setFeaturesCol('non_norm_features').fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = lda_model.transform(tfidf).drop('non_norm_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.write.parquet('result_lda.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the text file and remove the first three rows (zip trick)\n",
    "wlp_rdd = sc.textFile('../*-*-*.txt').zipWithIndex().filter(lambda r: r[1] > 2).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+-------+-----------+\n",
      "|     idseq|      lemma|    pos| textID|       word|\n",
      "+----------+-----------+-------+-------+-----------+\n",
      "|2654351732|   official|    nn2|1787313|  officials|\n",
      "|2654351733|       have|    vh0|1787313|       have|\n",
      "|2654351734|     accuse|    vvn|1787313|    accused|\n",
      "|2654351735|       mine|vvg_nn1|1787313|     mining|\n",
      "|2654351736|       firm|    nn2|1787313|      firms|\n",
      "|2654351737|         of|     io|1787313|         of|\n",
      "|2654351738|      react|    vvg|1787313|   reacting|\n",
      "|2654351739|       like|     ii|1787313|       like|\n",
      "|2654351740|    spoiled|    jj@|1787313|    spoiled|\n",
      "|2654351741|      child|    nn2|1787313|   children|\n",
      "|2654351742|           |      ,|1787313|          ,|\n",
      "|2654351743|        but|    ccb|1787313|        but|\n",
      "|2654351744|        the|     at|1787313|        the|\n",
      "|2654351745|  tanzanian|     jj|1787313|  Tanzanian|\n",
      "|2654351746| government|    nn1|1787313| government|\n",
      "|2654351747|         's|     ge|1787313|         's|\n",
      "|2654351748|   approach|    nn1|1787313|   approach|\n",
      "|2654351749|         to|     ii|1787313|         to|\n",
      "|2654351750|       mine|vvg_nn1|1787313|     mining|\n",
      "|2654351751|legislation|    nn1|1787313|legislation|\n",
      "+----------+-----------+-------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we split the elements separated by tabs\n",
    "lines = wlp_rdd.map(lambda r: r.split('\\t'))\n",
    "\n",
    "#identify the columns\n",
    "wlp_schema = lines.map(lambda r: Row(textID=int(r[0]),idseq=int(r[1]),word=r[2],lemma=r[3],pos=r[4]))\n",
    "wlp = spark.createDataFrame(wlp_schema)\n",
    "wlp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| textID|count|\n",
      "+-------+-----+\n",
      "|1787820|  169|\n",
      "|1787313|  846|\n",
      "|1787819|  386|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wlp.groupBy('textID').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_remove = ['.',',',\"\\'\",'\\\"','null']\n",
    "wlp_nopos = wlp.filter(~wlp['pos'].isin(pos_remove)).filter(~wlp['pos'].startswith('m')).filter(~wlp['pos'].startswith('f')).drop('idseq','pos','word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  5639\n"
     ]
    }
   ],
   "source": [
    "#np.save('our_stopwords',stopwords)\n",
    "stopwords = sc.textFile('../our_stopwords.txt').collect()\n",
    "print('Number of stopwords: ', len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      lemma|count|\n",
      "+-----------+-----+\n",
      "|       mine|   10|\n",
      "|   tanzania|    9|\n",
      "|     mining|    8|\n",
      "| government|    6|\n",
      "|     cookie|    6|\n",
      "|        fee|    6|\n",
      "|    royalty|    6|\n",
      "|     public|    6|\n",
      "|legislation|    6|\n",
      "|   industry|    6|\n",
      "|     device|    6|\n",
      "| investment|    6|\n",
      "|      astro|    5|\n",
      "|     change|    5|\n",
      "|   investor|    5|\n",
      "|     sector|    5|\n",
      "|        use|    5|\n",
      "|  tanzanian|    5|\n",
      "|       high|    4|\n",
      "|   minister|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter out stopwords and looking at the frequency of words without them\n",
    "wlp_nostop = wlp_nopos.filter(~wlp['lemma'].isin(stopwords))\n",
    "lemma_freq = wlp_nostop.groupBy('lemma').count().sort('count', ascending=False)\n",
    "lemma_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemmas left: 46\n",
      "Percentage of lemmas left: 12.81\n"
     ]
    }
   ],
   "source": [
    "#calculate percentiles and filtering out the lemmas above and below them\n",
    "[bottom,top] = lemma_freq.approxQuantile('count', [0.8,0.99], 0.01)\n",
    "lemma_tokeep = lemma_freq.filter(lemma_freq['count']>bottom).filter(lemma_freq['count']<top)\n",
    "c = lemma_tokeep.count()\n",
    "print('Number of lemmas left: %d'%c)\n",
    "print('Percentage of lemmas left: %.2f'%(c/lemma_freq.count()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "| textID|          lemma_list|\n",
      "+-------+--------------------+\n",
      "|1787313|[subject, subject...|\n",
      "|1787819|[subject, set, re...|\n",
      "|1787820|[astro, astro, as...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perform sql query and inner join\n",
    "wlp_nostop.registerTempTable('wlp_nostop')\n",
    "lemma_tokeep.registerTempTable('lemma_tokeep')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT wlp_nostop.lemma, wlp_nostop.textID\n",
    "FROM wlp_nostop\n",
    "INNER JOIN lemma_tokeep ON wlp_nostop.lemma = lemma_tokeep.lemma\n",
    "\"\"\"\n",
    "\n",
    "wlp_kept = spark.sql(query)\n",
    "wlp_bytext = wlp_kept.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "                    .sort('textID')\\\n",
    "                    .withColumnRenamed('collect_list(lemma)','lemma_list')\n",
    "wlp_bytext.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
