{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.parquet('tfidf_all.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "| textID|            features|\n",
      "+-------+--------------------+\n",
      "|1787313|(335,[0,2,4,5,7,8...|\n",
      "|1787819|(335,[1,2,3,4,6,9...|\n",
      "|1787820|(335,[18,20,22,23...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.rdd.map(lambda r: r[1].norm(2)).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = sc.textFile('voc.txt').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the text file and remove the first three rows (zip trick)\n",
    "wlp_rdd = sc.textFile('../*-*-*.txt').zipWithIndex().filter(lambda r: r[1] > 2).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+-------+-----------+\n",
      "|     idseq|      lemma|    pos| textID|       word|\n",
      "+----------+-----------+-------+-------+-----------+\n",
      "|2654351732|   official|    nn2|1787313|  officials|\n",
      "|2654351733|       have|    vh0|1787313|       have|\n",
      "|2654351734|     accuse|    vvn|1787313|    accused|\n",
      "|2654351735|       mine|vvg_nn1|1787313|     mining|\n",
      "|2654351736|       firm|    nn2|1787313|      firms|\n",
      "|2654351737|         of|     io|1787313|         of|\n",
      "|2654351738|      react|    vvg|1787313|   reacting|\n",
      "|2654351739|       like|     ii|1787313|       like|\n",
      "|2654351740|    spoiled|    jj@|1787313|    spoiled|\n",
      "|2654351741|      child|    nn2|1787313|   children|\n",
      "|2654351742|           |      ,|1787313|          ,|\n",
      "|2654351743|        but|    ccb|1787313|        but|\n",
      "|2654351744|        the|     at|1787313|        the|\n",
      "|2654351745|  tanzanian|     jj|1787313|  Tanzanian|\n",
      "|2654351746| government|    nn1|1787313| government|\n",
      "|2654351747|         's|     ge|1787313|         's|\n",
      "|2654351748|   approach|    nn1|1787313|   approach|\n",
      "|2654351749|         to|     ii|1787313|         to|\n",
      "|2654351750|       mine|vvg_nn1|1787313|     mining|\n",
      "|2654351751|legislation|    nn1|1787313|legislation|\n",
      "+----------+-----------+-------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we split the elements separated by tabs\n",
    "lines = wlp_rdd.map(lambda r: r.split('\\t'))\n",
    "\n",
    "#identify the columns\n",
    "wlp_schema = lines.map(lambda r: Row(textID=int(r[0]),idseq=int(r[1]),word=r[2],lemma=r[3],pos=r[4]))\n",
    "wlp = spark.createDataFrame(wlp_schema)\n",
    "wlp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| textID|count|\n",
      "+-------+-----+\n",
      "|1787820|  169|\n",
      "|1787313|  846|\n",
      "|1787819|  386|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wlp.groupBy('textID').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_remove = ['.',',',\"\\'\",'\\\"','null']\n",
    "wlp_nopos = wlp.filter(~wlp['pos'].isin(pos_remove)).filter(~wlp['pos'].startswith('m')).filter(~wlp['pos'].startswith('f')).drop('idseq','pos','word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  5639\n"
     ]
    }
   ],
   "source": [
    "#np.save('our_stopwords',stopwords)\n",
    "stopwords = sc.textFile('../our_stopwords.txt').collect()\n",
    "print('Number of stopwords: ', len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      lemma|count|\n",
      "+-----------+-----+\n",
      "|       mine|   10|\n",
      "|   tanzania|    9|\n",
      "|     mining|    8|\n",
      "| government|    6|\n",
      "|     cookie|    6|\n",
      "|        fee|    6|\n",
      "|    royalty|    6|\n",
      "|     public|    6|\n",
      "|legislation|    6|\n",
      "|   industry|    6|\n",
      "|     device|    6|\n",
      "| investment|    6|\n",
      "|      astro|    5|\n",
      "|     change|    5|\n",
      "|   investor|    5|\n",
      "|     sector|    5|\n",
      "|        use|    5|\n",
      "|  tanzanian|    5|\n",
      "|       high|    4|\n",
      "|   minister|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter out stopwords and looking at the frequency of words without them\n",
    "wlp_nostop = wlp_nopos.filter(~wlp['lemma'].isin(stopwords))\n",
    "lemma_freq = wlp_nostop.groupBy('lemma').count().sort('count', ascending=False)\n",
    "lemma_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemmas left: 46\n",
      "Percentage of lemmas left: 12.81\n"
     ]
    }
   ],
   "source": [
    "#calculate percentiles and filtering out the lemmas above and below them\n",
    "[bottom,top] = lemma_freq.approxQuantile('count', [0.8,0.99], 0.01)\n",
    "lemma_tokeep = lemma_freq.filter(lemma_freq['count']>bottom).filter(lemma_freq['count']<top)\n",
    "c = lemma_tokeep.count()\n",
    "print('Number of lemmas left: %d'%c)\n",
    "print('Percentage of lemmas left: %.2f'%(c/lemma_freq.count()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "| textID|          lemma_list|\n",
      "+-------+--------------------+\n",
      "|1787313|[subject, subject...|\n",
      "|1787819|[subject, set, re...|\n",
      "|1787820|[astro, astro, as...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perform sql query and inner join\n",
    "wlp_nostop.registerTempTable('wlp_nostop')\n",
    "lemma_tokeep.registerTempTable('lemma_tokeep')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT wlp_nostop.lemma, wlp_nostop.textID\n",
    "FROM wlp_nostop\n",
    "INNER JOIN lemma_tokeep ON wlp_nostop.lemma = lemma_tokeep.lemma\n",
    "\"\"\"\n",
    "\n",
    "wlp_kept = spark.sql(query)\n",
    "wlp_bytext = wlp_kept.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "                    .sort('textID')\\\n",
    "                    .withColumnRenamed('collect_list(lemma)','lemma_list')\n",
    "wlp_bytext.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
