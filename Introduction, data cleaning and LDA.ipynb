{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LDA and data cleaning\n",
    "In this notebook, we introduce LDA and what we need for our model. We then proceed to load and clean a sample of the NOW corpus to fulfill our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LDA\n",
    "[Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a statistical model which we will use for topic modelling/discovery. LDA will, given a list of words belonging to a text, output the topics present and their probability. In here, a topic is represented as a probability distribution of words. Thus each text/document will be a distribution over the topics. In short, texts have an associated topic distribution and topics have a word distribution. \n",
    "\n",
    "The image below is the plate notation for LDA, where:\n",
    "* θ<sub>m</sub> is the topic distribution for document m,\n",
    "* φ<sub>k</sub> is the word distribution for topic k,\n",
    "* z<sub>mn</sub> is the topic for the n-th word in document m, and\n",
    "* w<sub>mn</sub> is the specific word.\n",
    "* α is the parameter of the Dirichlet prior on the per-document topic distributions,\n",
    "* β is the parameter of the Dirichlet prior on the per-topic word distribution,\n",
    "\n",
    "![](LDA.png)\n",
    "\n",
    "α and β are the parameters for the model. A big α means that documents are likely to be represented by a high number of topics and vice versa. Same goes for β, a high value meaning that topics are represented by a hign number of words. The number of topics that LDA outputs is dependent on our input and works a bit like clustering. If we allow too many topics we might end up splitting topics uselessly and a too few will make us group them unnecessarily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NOW corpus\n",
    "This notebook shows the cleaning process that will be used for the ADA project. Here, only a sample of the data is used (from [here](https://www.corpusdata.org/now_corpus.asp)), but the methods should be the same once scaled to the full database available on the cluster.\n",
    "\n",
    "The NOW database is composed of billions of words from online newspapers and magazines from 20 different countries. The data we downloaded comes in different files which can be used together or independently. These files are:\n",
    "\n",
    "1. **now-samples-lexicon.txt**: this is the full dictionnary of the english language, a lexicon. It contains four clolumns, `wID` which is the word id, `word` the actual word, `lemma` which is family of the word (ie: if word is \"walked\", lemma is \"walk\") and `PoS` which is the part of speech.\n",
    "2. **now-samples_sources.txt**: this is the source of every text, in order it contains the text id, the number of words, the date, the country, the website, the url and title of the article.\n",
    "3. **text.txt**: this file has the complete texts of the articles, the first column is the `textID` in the format @@textID, the second column is the full text, complete with html paragraphs and headers. It is important to note that to prevent plagiarism, every 200 words, 10 words are replaced by the string \"@ @ @ @ @ @ @ @ @ @\". Combined words are also split, example \"can't\" is written as \"ca n't\" and punctuation is surrounded by spaces.\n",
    "4. **wordLemPoS.txt**: finally, this file contains the `word`, `lemma` and `PoS` for each word in the texts, one by one, so one could read the texts by reading down the columns. Along with that is the `textID` from where the word is and an `ID (seq)` which is a unique indetifier for each word in the database. Each time a word is added this number is incremented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need from the NOW corpus for LDA\n",
    "The model will take two inputs, a matrix with all the important words for each text, and a list of all the important words. By important, it is meant the words which will give us good topic modelling. For example, names, locations, simple words like \"but, \"I\" or \"and\" will not give meaningfull results and are quite common in english (so-called stopwords). Other common words present in our database should be removed too. We also should use lemmas instead of words.\n",
    "\n",
    "Therefore, the file `wordLemPoS.txt` (hence referred as wlp) is the most important here as it lists all the lemmas with their `textID` associated. Which means that with it we can lsit all the lemmas, remove those we do not want to make our word list, but also group them by texts to create our text-word matrix.\n",
    "\n",
    "We will also need `now-sample_sources.txt` (hence referred as sources) to link the texts with the information we will deem useful. For example country, date or website.\n",
    "\n",
    "These are thus the two file we will import and process here with the sample data but also those we will use with the data on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wlp processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to extract the useful data from wlp text files. Since they contain all the words of all the articles and the lemmas to replace them with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read the text file\n",
    "wlp_rdd = sc.textFile('sample_data/wordLem_poS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first 3 lines are useless headlines\n",
    "header = wlp_rdd.take(3)\n",
    "\n",
    "#so let's remove those headlines\n",
    "noheaders = wlp_rdd.filter(lambda r: r != header[0])\\\n",
    "            .filter(lambda r: r != header[1])\\\n",
    "            .filter(lambda r: r != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------+-------+\n",
      "|     idseq| lemma| pos|textID|   word|\n",
      "+----------+------+----+------+-------+\n",
      "|1095362496|      |  fo| 11241|@@11241|\n",
      "|1095362497|      |null| 11241|    <p>|\n",
      "|1095362498|   sol| np1| 11241|    Sol|\n",
      "|1095362499|yurick| np1| 11241| Yurick|\n",
      "|1095362500|      |   ,| 11241|      ,|\n",
      "+----------+------+----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we split the elements separated by tabs\n",
    "lines = noheaders.map(lambda r: r.split('\\t'))\n",
    "\n",
    "#identify the columns\n",
    "wlp_schema = lines.map(lambda r: Row(textID=int(r[0]),idseq=int(r[1]),word=r[2],lemma=r[3],pos=r[4]))\n",
    "wlp = spark.createDataFrame(wlp_schema)\n",
    "wlp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word selection\n",
    "It is very important to select the right words and the right number. The ocncept of \"garbage in garbage out\" has never been more true than with LDA. When we analyse a text we focus on certain words to extract it's meaning and topic. The same is true here since words like if, for, numbers, common names are not that useful.\n",
    "\n",
    "Here, we provide and example of the process we will go through. However this is not really a data cleaning step as it will directly influence our model. It is more of a model preprocessing step. We will surely go through many iterations of this next part for our model to give the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can remove all the words which have a PoS which do not interest us. For example number (`mc`,`mc1`,`m#`) or punctuation (`.`,`'`), etc...\n",
    "\n",
    "Details:\n",
    "1. `.`,`,`,`'` and `\"` are punctuations\n",
    "2. `null` are html tags from the websites\n",
    "3. `mc`,`mc1` and `m#` are various numbers\n",
    "3. `fo`, `fu` are random non sensical words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_remove = ['.',',',\"\\'\",'\\\"','null','mc','mc1','m#','fo','fu']\n",
    "wlp_nopos = wlp.filter(~wlp['pos'].isin(pos_remove)).drop('idseq','pos','word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load our list of stopwords, the words that we are not going to use in LDA as they are too common or are common names. We can also remove the rows with no lemmas or those with lemmas that don't make sense or are not common enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  5639\n"
     ]
    }
   ],
   "source": [
    "#np.save('our_stopwords',stopwords)\n",
    "stopwords = np.load('our_stopwords.npy').tolist()\n",
    "print('Number of stopwords: ', len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     lemma|count|\n",
      "+----------+-----+\n",
      "|      year| 4272|\n",
      "|      time| 3169|\n",
      "|    people| 2913|\n",
      "|      take| 2667|\n",
      "|       use| 2244|\n",
      "|      work| 2137|\n",
      "|       day| 1819|\n",
      "|     state| 1713|\n",
      "|   company| 1698|\n",
      "|   comment| 1667|\n",
      "|      need| 1654|\n",
      "|      want| 1579|\n",
      "|      look| 1564|\n",
      "|     world| 1553|\n",
      "|government| 1551|\n",
      "|      show| 1480|\n",
      "|      give| 1480|\n",
      "|   country| 1465|\n",
      "|      find| 1464|\n",
      "|     right| 1408|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter out stopwords and looking at the frequency of words without them\n",
    "wlp_nostop = wlp_nopos.filter(~wlp['lemma'].isin(stopwords))\n",
    "lemma_freq = wlp_nostop.groupBy('lemma').count().sort('count', ascending=False)\n",
    "lemma_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove the most common and least common lemmas. These will be useless since they won't provide enough information for our LDA analysis. Here, we filter out the top 5% and bottom 10% of all lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Maybe should change from percentile to number for bottom filtering, depending on which one is the harshest (in here it is number)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of lemmas left: 28.47\n"
     ]
    }
   ],
   "source": [
    "#calculate percentiles and filtering out the lemmas above and below them\n",
    "[bottom,top] = lemma_freq.approxQuantile('count', [0.1,0.99], 0.01)\n",
    "bottom = 5\n",
    "lemma_tokeep = lemma_freq.filter(lemma_freq['count']>bottom).filter(lemma_freq['count']<top)\n",
    "print('Percentage of lemmas left: %.2f'%(lemma_tokeep.count()/lemma_freq.count()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a inner join, we keep only the words which are in both lists! In the end, we can group the lemmas in their texts to create our text-word matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textID|          lemma_list|\n",
      "+------+--------------------+\n",
      "| 11241|[1970s, film, fil...|\n",
      "| 11242|[online, happen, ...|\n",
      "| 11243|[dough, dough, do...|\n",
      "| 11244|[trail, launch, o...|\n",
      "| 21242|[online, launch, ...|\n",
      "| 21243|[recognize, indic...|\n",
      "| 31240|[recognize, inten...|\n",
      "| 31241|[online, online, ...|\n",
      "| 31242|[settlement, sett...|\n",
      "| 41240|[explain, hometow...|\n",
      "| 41241|[scale, lack, pre...|\n",
      "| 41244|[everyday, trail,...|\n",
      "| 51243|[australia, austr...|\n",
      "| 61240|[frustrate, inten...|\n",
      "| 61242|[editor-in-chief,...|\n",
      "| 71240|[indicator, requi...|\n",
      "| 71241|[likelihood, requ...|\n",
      "| 71242|[1970s, character...|\n",
      "| 71243|[online, staff, s...|\n",
      "| 71244|[bone, archaeolog...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perform sql query and inner join\n",
    "wlp_nostop.registerTempTable('wlp_nostop')\n",
    "lemma_tokeep.registerTempTable('lemma_tokeep')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT wlp_nostop.lemma, wlp_nostop.textID\n",
    "FROM wlp_nostop\n",
    "INNER JOIN lemma_tokeep ON wlp_nostop.lemma = lemma_tokeep.lemma\n",
    "\"\"\"\n",
    "\n",
    "wlp_kept = spark.sql(query)\n",
    "wlp_bytext = wlp_kept.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "                    .sort('textID')\\\n",
    "                    .withColumnRenamed('collect_list(lemma)','lemma_list')\n",
    "wlp_bytext.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "Contains all the additional informations about each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_rdd = sc.textFile('sample_data/now-samples-sources.txt')\\\n",
    "                .map(lambda r: r.split('\\t'))\n",
    "\n",
    "header = sources_rdd.take(3)\n",
    "sources_rdd = sources_rdd.filter(lambda l: l != header[0])\\\n",
    "                .filter(lambda l: l != header[1])\\\n",
    "                .filter(lambda l: l != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create schema and change data type for date\n",
    "sources_schema = sources_rdd.map(lambda r: Row(textID=int(r[0]),nwords=int(r[1]),date=r[2],country=r[3],website=r[4],url=r[5],title=r[6],)) \n",
    "sources = spark.createDataFrame(sources_schema)\n",
    "sources = sources.withColumn('date',to_date(sources.date, 'yy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- nwords: long (nullable = true)\n",
      " |-- textID: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|country|      date|nwords|textID|               title|                 url|            website|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|     US|2013-01-06|   397| 11241|Author of The War...|http://kotaku.com...|             Kotaku|\n",
      "|     US|2013-01-06|   757| 11242|That's What They ...|http://michiganra...|     Michigan Radio|\n",
      "|     US|2013-01-06|   755| 11243|Best of New York:...|http://www.nydail...|New York Daily News|\n",
      "|     US|2013-01-06|  1677| 11244|Reflecting on a q...|http://www.oregon...|     OregonLive.com|\n",
      "|     US|2013-01-11|   794| 21242|Ask Ars: Does Fac...|http://arstechnic...|       Ars Technica|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "This is an expanded version on parrot's LDA using spark only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "#from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can set certain filtering with CountVectorizer, not sure if that's useful since we already do that before hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|textID|          lemma_list|        raw_features|\n",
      "+------+--------------------+--------------------+\n",
      "| 11241|[1970s, film, fil...|(11399,[4,14,15,1...|\n",
      "| 11242|[online, happen, ...|(11399,[0,1,11,16...|\n",
      "| 11243|[dough, dough, do...|(11399,[0,2,3,4,5...|\n",
      "| 11244|[trail, launch, o...|(11399,[0,2,4,5,6...|\n",
      "| 21242|[online, launch, ...|(11399,[0,1,2,3,5...|\n",
      "| 21243|[recognize, indic...|(11399,[3,4,5,6,7...|\n",
      "| 31240|[recognize, inten...|(11399,[0,5,10,12...|\n",
      "| 31241|[online, online, ...|(11399,[2,9,12,30...|\n",
      "| 31242|[settlement, sett...|(11399,[6,8,36,50...|\n",
      "| 41240|[explain, hometow...|(11399,[0,1,2,4,5...|\n",
      "| 41241|[scale, lack, pre...|(11399,[1,3,4,11,...|\n",
      "| 41244|[everyday, trail,...|(11399,[0,1,3,4,7...|\n",
      "| 51243|[australia, austr...|(11399,[0,1,3,4,5...|\n",
      "| 61240|[frustrate, inten...|(11399,[1,10,25,2...|\n",
      "| 61242|[editor-in-chief,...|(11399,[0,1,2,7,9...|\n",
      "| 71240|[indicator, requi...|(11399,[9,13,14,1...|\n",
      "| 71241|[likelihood, requ...|(11399,[3,9,11,13...|\n",
      "| 71242|[1970s, character...|(11399,[0,2,3,7,9...|\n",
      "| 71243|[online, staff, s...|(11399,[0,1,2,9,1...|\n",
      "| 71244|[bone, archaeolog...|(11399,[0,1,4,10,...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvmodel = CountVectorizer(inputCol=\"lemma_list\", outputCol=\"raw_features\").fit(wlp_bytext)\n",
    "result_cv = cvmodel.transform(wlp_bytext)\n",
    "result_cv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|textID|          lemma_list|        raw_features|            features|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "| 11241|[1970s, film, fil...|(11399,[4,14,15,1...|(11399,[4,14,15,1...|\n",
      "| 11242|[online, happen, ...|(11399,[0,1,11,16...|(11399,[0,1,11,16...|\n",
      "| 11243|[dough, dough, do...|(11399,[0,2,3,4,5...|(11399,[0,2,3,4,5...|\n",
      "| 11244|[trail, launch, o...|(11399,[0,2,4,5,6...|(11399,[0,2,4,5,6...|\n",
      "| 21242|[online, launch, ...|(11399,[0,1,2,3,5...|(11399,[0,1,2,3,5...|\n",
      "| 21243|[recognize, indic...|(11399,[3,4,5,6,7...|(11399,[3,4,5,6,7...|\n",
      "| 31240|[recognize, inten...|(11399,[0,5,10,12...|(11399,[0,5,10,12...|\n",
      "| 31241|[online, online, ...|(11399,[2,9,12,30...|(11399,[2,9,12,30...|\n",
      "| 31242|[settlement, sett...|(11399,[6,8,36,50...|(11399,[6,8,36,50...|\n",
      "| 41240|[explain, hometow...|(11399,[0,1,2,4,5...|(11399,[0,1,2,4,5...|\n",
      "| 41241|[scale, lack, pre...|(11399,[1,3,4,11,...|(11399,[1,3,4,11,...|\n",
      "| 41244|[everyday, trail,...|(11399,[0,1,3,4,7...|(11399,[0,1,3,4,7...|\n",
      "| 51243|[australia, austr...|(11399,[0,1,3,4,5...|(11399,[0,1,3,4,5...|\n",
      "| 61240|[frustrate, inten...|(11399,[1,10,25,2...|(11399,[1,10,25,2...|\n",
      "| 61242|[editor-in-chief,...|(11399,[0,1,2,7,9...|(11399,[0,1,2,7,9...|\n",
      "| 71240|[indicator, requi...|(11399,[9,13,14,1...|(11399,[9,13,14,1...|\n",
      "| 71241|[likelihood, requ...|(11399,[3,9,11,13...|(11399,[3,9,11,13...|\n",
      "| 71242|[1970s, character...|(11399,[0,2,3,7,9...|(11399,[0,2,3,7,9...|\n",
      "| 71243|[online, staff, s...|(11399,[0,1,2,9,1...|(11399,[0,1,2,9,1...|\n",
      "| 71244|[bone, archaeolog...|(11399,[0,1,4,10,...|(11399,[0,1,4,10,...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfModel = IDF(inputCol=\"raw_features\", outputCol=\"features\").fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) \n",
    "result_tfidf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 230 ms, sys: 80.9 ms, total: 310 ms\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = LDA(k=10, maxIter=10).fit(result_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:50% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#evaluate this for wider cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:50% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe topics.\n",
    "topics = lda_model.describeTopics()\n",
    "voc = np.array(cvmodel.vocabulary)\n",
    "words_topics = topics.toPandas().termIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "['school' 'bc' 'patient' 'medical' 'education' 'courier' 'student'\n",
      " 'doctor' 'anwar' 'village']\n",
      "Topic 1\n",
      "['pakistan' 'khan' 'oct' 'beer' 'penalty' 'salman' 'market' 'migrant'\n",
      " 'camp' 'football']\n",
      "Topic 2\n",
      "['vegas' 'ball' 'cricket' 'pecos' 'shenandoah' 'la' 'league'\n",
      " 'review-journal' 'match' 'liverpool']\n",
      "Topic 3\n",
      "['health' 'gas' 'cancer' 'antibiotic' 'hepatitis' 'animal' 'dr' 'island'\n",
      " 'species' 'possum']\n",
      "Topic 4\n",
      "['game' 'team' 'race' 'bank' 'police' 'win' 'look' 'play' 'party' 'coach']\n",
      "Topic 5\n",
      "['people' 'tax' 'company' 'canada' 'want' 'public' 'use' 'drug' 'child'\n",
      " 'government']\n",
      "Topic 6\n",
      "['market' 'quebec' 'porn' 'comment' 'trading' 'low' 'investor' 'industry'\n",
      " 'content' 'product']\n",
      "Topic 7\n",
      "['*' 'muslim' 'police' 'app' 'share' 'harley-davidson' 'company' 'quarter'\n",
      " 'islam' 'indian']\n",
      "Topic 8\n",
      "['dog' 'movie' 'city' 'film' 'artist' 'waterfront' 'bond' 'project'\n",
      " 'refugee' 'zealand']\n",
      "Topic 9\n",
      "['spill' 'cork' 'military' 'boat' '*' 'syria' 'missile' 'russian'\n",
      " 'aircraft' 'oil']\n"
     ]
    }
   ],
   "source": [
    "for t in range(len(words_topics)):\n",
    "    print('Topic', t)\n",
    "    print(voc[words_topics[t]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distr = lda_model.transform(result_tfidf).drop('lemma_list','raw_features','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "7\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "8\n",
      "5\n",
      "1\n",
      "0\n",
      "6\n",
      "5\n",
      "8\n",
      "0\n",
      "9\n",
      "7\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "8\n",
      "8\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "0\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "9\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "6\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "2\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "7\n",
      "5\n",
      "3\n",
      "4\n",
      "9\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "1\n",
      "9\n",
      "7\n",
      "5\n",
      "6\n",
      "5\n",
      "9\n",
      "7\n",
      "5\n",
      "6\n",
      "7\n",
      "6\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "2\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "1\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "7\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "9\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "7\n",
      "4\n",
      "8\n",
      "0\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "9\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "9\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "9\n",
      "5\n",
      "0\n",
      "1\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "7\n",
      "4\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "4\n",
      "6\n",
      "5\n",
      "0\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "0\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "4\n",
      "8\n",
      "5\n",
      "6\n",
      "5\n",
      "8\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "9\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "7\n",
      "8\n",
      "9\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "7\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "9\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "9\n",
      "7\n",
      "7\n",
      "9\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "1\n",
      "4\n",
      "1\n",
      "5\n",
      "6\n",
      "7\n",
      "5\n",
      "5\n",
      "7\n",
      "6\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "8\n",
      "8\n",
      "4\n",
      "2\n",
      "6\n",
      "4\n",
      "9\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "7\n",
      "6\n",
      "8\n",
      "5\n",
      "9\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "8\n",
      "9\n",
      "7\n",
      "7\n",
      "7\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "5\n",
      "2\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "4\n",
      "9\n",
      "5\n",
      "5\n",
      "0\n",
      "9\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "8\n",
      "6\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "0\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "4\n",
      "8\n",
      "7\n",
      "5\n",
      "8\n",
      "5\n",
      "3\n",
      "1\n",
      "1\n",
      "7\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "2\n",
      "8\n",
      "4\n",
      "1\n",
      "5\n",
      "5\n",
      "0\n",
      "4\n",
      "9\n",
      "5\n",
      "0\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "9\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "3\n",
      "8\n",
      "3\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "0\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "9\n",
      "5\n",
      "5\n",
      "2\n",
      "2\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "2\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "0\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "7\n",
      "4\n",
      "7\n",
      "4\n",
      "9\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "1\n",
      "4\n",
      "8\n",
      "7\n",
      "8\n",
      "4\n",
      "5\n",
      "7\n",
      "7\n",
      "6\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "6\n",
      "8\n",
      "4\n",
      "6\n",
      "4\n",
      "8\n",
      "6\n",
      "8\n",
      "8\n",
      "6\n",
      "9\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "0\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "7\n",
      "3\n",
      "0\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "9\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "8\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "6\n",
      "1\n",
      "7\n",
      "5\n",
      "8\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "6\n",
      "7\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "9\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "1\n",
      "4\n",
      "2\n",
      "9\n",
      "7\n",
      "5\n",
      "7\n",
      "2\n",
      "1\n",
      "6\n",
      "5\n",
      "8\n",
      "8\n",
      "5\n",
      "7\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "7\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "9\n",
      "9\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "9\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "0\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "8\n",
      "5\n",
      "7\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "3\n",
      "1\n",
      "8\n",
      "8\n",
      "5\n",
      "8\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "1\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "8\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "5\n",
      "8\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "7\n",
      "4\n",
      "6\n",
      "7\n",
      "2\n",
      "1\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "2\n",
      "5\n",
      "8\n",
      "2\n",
      "6\n",
      "9\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "5\n",
      "7\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "1\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "0\n",
      "1\n",
      "8\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "2\n",
      "4\n",
      "7\n",
      "5\n",
      "2\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "0\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "8\n",
      "8\n",
      "9\n",
      "4\n",
      "6\n",
      "2\n",
      "4\n",
      "5\n",
      "9\n",
      "6\n",
      "5\n",
      "5\n",
      "7\n",
      "1\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "0\n",
      "3\n",
      "9\n",
      "4\n",
      "2\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "1\n",
      "1\n",
      "7\n",
      "5\n",
      "5\n",
      "7\n",
      "9\n",
      "6\n",
      "4\n",
      "2\n",
      "4\n",
      "4\n",
      "8\n",
      "4\n",
      "5\n",
      "9\n",
      "4\n",
      "0\n",
      "4\n",
      "9\n",
      "5\n",
      "5\n",
      "6\n",
      "8\n",
      "6\n",
      "2\n",
      "1\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "4\n",
      "8\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "8\n",
      "4\n",
      "8\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "8\n",
      "9\n",
      "7\n",
      "5\n",
      "9\n",
      "4\n",
      "9\n",
      "4\n",
      "8\n",
      "6\n",
      "5\n",
      "9\n",
      "5\n",
      "1\n",
      "2\n",
      "9\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "1\n",
      "7\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "8\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "6\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "8\n",
      "2\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "9\n",
      "4\n",
      "7\n",
      "2\n",
      "5\n",
      "5\n",
      "9\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "6\n",
      "8\n",
      "5\n",
      "4\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "5\n",
      "2\n",
      "4\n",
      "1\n",
      "4\n",
      "8\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "9\n",
      "5\n",
      "6\n",
      "7\n",
      "6\n",
      "8\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "6\n",
      "3\n",
      "4\n",
      "8\n",
      "4\n",
      "0\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "7\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "2\n",
      "5\n",
      "9\n",
      "5\n",
      "8\n",
      "1\n",
      "5\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "9\n",
      "4\n",
      "5\n",
      "2\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "1\n",
      "8\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "8\n",
      "8\n",
      "8\n",
      "5\n",
      "8\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "7\n",
      "8\n",
      "4\n",
      "7\n",
      "7\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "6\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "2\n",
      "2\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0\n",
      "5\n",
      "0\n",
      "6\n",
      "0\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "6\n",
      "0\n",
      "2\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "1\n",
      "1\n",
      "7\n",
      "3\n",
      "8\n",
      "4\n",
      "9\n",
      "8\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "8\n",
      "2\n",
      "4\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "7\n",
      "4\n",
      "1\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "2\n",
      "5\n",
      "4\n",
      "4\n",
      "1\n",
      "8\n",
      "4\n",
      "2\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "7\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "4\n",
      "5\n",
      "8\n",
      "2\n",
      "5\n",
      "5\n",
      "6\n",
      "9\n",
      "4\n",
      "5\n",
      "2\n",
      "2\n",
      "6\n",
      "8\n",
      "4\n",
      "0\n",
      "8\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "6\n",
      "7\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "8\n",
      "9\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "6\n",
      "7\n",
      "6\n",
      "7\n",
      "8\n",
      "5\n",
      "7\n",
      "2\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "8\n",
      "6\n",
      "5\n",
      "5\n",
      "9\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "8\n",
      "0\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "5\n",
      "9\n",
      "5\n",
      "1\n",
      "4\n",
      "8\n",
      "9\n",
      "5\n",
      "9\n",
      "5\n",
      "3\n",
      "5\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "2\n",
      "9\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "8\n",
      "8\n",
      "9\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "7\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "9\n",
      "7\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "4\n",
      "8\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "9\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "8\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "7\n",
      "7\n",
      "4\n",
      "4\n",
      "7\n",
      "1\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "7\n",
      "4\n",
      "4\n",
      "1\n",
      "1\n",
      "5\n",
      "0\n",
      "5\n",
      "3\n",
      "2\n",
      "2\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "9\n",
      "5\n",
      "7\n",
      "6\n",
      "6\n",
      "5\n",
      "3\n",
      "8\n",
      "9\n",
      "7\n",
      "6\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "2\n",
      "4\n",
      "6\n",
      "7\n",
      "4\n",
      "8\n",
      "4\n",
      "3\n",
      "5\n",
      "9\n",
      "8\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "1\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "9\n",
      "5\n",
      "7\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "4\n",
      "7\n",
      "8\n",
      "8\n",
      "1\n",
      "4\n",
      "4\n",
      "5\n",
      "0\n",
      "0\n",
      "7\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "1\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "8\n",
      "3\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "8\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "2\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "7\n",
      "4\n",
      "7\n",
      "2\n",
      "5\n",
      "5\n",
      "0\n",
      "6\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "6\n",
      "8\n",
      "5\n",
      "5\n",
      "1\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "9\n",
      "2\n",
      "5\n",
      "0\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "8\n",
      "8\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "6\n",
      "3\n",
      "5\n",
      "8\n",
      "7\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "8\n",
      "3\n",
      "4\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "9\n",
      "5\n",
      "5\n",
      "1\n",
      "6\n",
      "5\n",
      "4\n",
      "2\n",
      "5\n",
      "5\n",
      "7\n",
      "6\n",
      "4\n",
      "4\n",
      "7\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "9\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "4\n",
      "9\n",
      "5\n",
      "4\n",
      "2\n",
      "4\n",
      "8\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "5\n",
      "2\n",
      "9\n",
      "1\n",
      "5\n",
      "5\n",
      "6\n",
      "1\n",
      "7\n",
      "7\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "2\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "6\n",
      "2\n",
      "6\n",
      "5\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "6\n",
      "1\n",
      "5\n",
      "2\n",
      "7\n",
      "0\n",
      "7\n",
      "5\n",
      "4\n",
      "9\n",
      "5\n",
      "0\n",
      "6\n",
      "4\n",
      "4\n",
      "9\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "2\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "5\n",
      "5\n",
      "9\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "6\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "2\n",
      "4\n",
      "5\n",
      "2\n",
      "2\n",
      "3\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "2\n",
      "4\n",
      "8\n",
      "2\n",
      "3\n",
      "8\n",
      "5\n",
      "2\n",
      "5\n",
      "8\n",
      "4\n",
      "4\n",
      "8\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "4\n",
      "1\n",
      "4\n",
      "5\n",
      "8\n",
      "8\n",
      "5\n",
      "3\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "4\n",
      "7\n",
      "0\n",
      "4\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "9\n",
      "1\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "5\n",
      "6\n",
      "8\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "7\n",
      "5\n",
      "3\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "1\n",
      "7\n",
      "3\n",
      "7\n",
      "6\n",
      "7\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "5\n",
      "2\n",
      "2\n",
      "6\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "5\n",
      "7\n",
      "5\n",
      "8\n",
      "9\n",
      "5\n",
      "0\n",
      "3\n",
      "8\n",
      "8\n",
      "0\n",
      "5\n",
      "3\n",
      "8\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "7\n",
      "8\n",
      "7\n",
      "4\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "2\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "8\n",
      "4\n",
      "2\n",
      "7\n",
      "6\n",
      "2\n",
      "7\n",
      "3\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "0\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "2\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "0\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "5\n",
      "2\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "2\n",
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "8\n",
      "4\n",
      "9\n",
      "7\n",
      "5\n",
      "5\n",
      "4\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "1\n",
      "8\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "5\n",
      "4\n",
      "2\n",
      "7\n",
      "7\n",
      "6\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "4\n",
      "1\n",
      "5\n",
      "2\n",
      "8\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "5\n",
      "8\n",
      "2\n",
      "4\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "1\n",
      "0\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "1\n",
      "4\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "4\n",
      "4\n",
      "5\n",
      "9\n",
      "0\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "7\n",
      "1\n",
      "7\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "2\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "2\n",
      "5\n",
      "6\n",
      "3\n",
      "3\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "1\n",
      "7\n",
      "7\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "5\n",
      "0\n",
      "5\n",
      "4\n",
      "5\n",
      "4\n",
      "6\n",
      "4\n",
      "8\n",
      "6\n",
      "4\n",
      "3\n",
      "5\n",
      "0\n",
      "7\n",
      "1\n",
      "4\n",
      "5\n",
      "9\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "0\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "0\n",
      "7\n",
      "6\n",
      "6\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "0\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "4\n",
      "5\n",
      "7\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "7\n",
      "5\n",
      "1\n",
      "1\n",
      "0\n",
      "5\n",
      "3\n",
      "5\n",
      "1\n",
      "8\n",
      "5\n",
      "5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for t in topic_distr.rdd.map(lambda r: r[1]).collect():\n",
    "    print(argmax(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "def argmax(x):\n",
    "    return np.argmax(x)\n",
    "argmax_udf = udf(lambda r: argmax(r), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o12781.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 704.0 failed 1 times, most recent failure: Lost task 5.0 in stage 704.0 (TID 30716, localhost, executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:86)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:85)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)\n\tat org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1430)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:86)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:85)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)\n\tat org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1430)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-6d696a35f6c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmain_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'textID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margmax_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topicDistribution'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int_squared'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Programs/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o12781.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 704.0 failed 1 times, most recent failure: Lost task 5.0 in stage 704.0 (TID 30716, localhost, executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:86)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:85)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)\n\tat org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1430)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:86)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:85)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)\n\tat org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1430)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "main_topic = topic_distr.select('textID',argmax_udf('topicDistribution').alias('int_squared'))\n",
    "main_topic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
