{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LDA and data cleaning\n",
    "In this notebook, we introduce LDA and what we need for our model. We then proceed to load and clean a sample of the NOW corpus to fulfill our needs.\n",
    "\n",
    "## What is LDA\n",
    "[Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a statistical model which we will use for topic modelling/discovery. LDA will, given a list of words belonging to a text, output the topics present and their probability. In here, a topic is represented as a probability distribution of words. Thus each text/document will be a distribution over the topics. In short, texts have an associated topic distribution and topics have a word distribution. \n",
    "\n",
    "The image below is the plate notation for LDA, where:\n",
    "* θ<sub>m</sub> is the topic distribution for document m,\n",
    "* φ<sub>k</sub> is the word distribution for topic k,\n",
    "* z<sub>mn</sub> is the topic for the n-th word in document m, and\n",
    "* w<sub>mn</sub> is the specific word.\n",
    "* α is the parameter of the Dirichlet prior on the per-document topic distributions,\n",
    "* β is the parameter of the Dirichlet prior on the per-topic word distribution,\n",
    "\n",
    "![](LDA.png)\n",
    "\n",
    "α and β are the parameters for the model. A big α means that documents are likely to be represented by a high number of topics and vice versa. Same goes for β, a high value meaning that topics are represented by a hign number of words. The number of topics that LDA outputs is dependent on our input and works a bit like clustering. If we allow too many topics we might end up splitting topics uselessly and a too few will make us group them unnecessarily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NOW corpus\n",
    "This notebook shows the cleaning process that will be used for the ADA project. Here, only a sample of the data is used (from [here](https://www.corpusdata.org/now_corpus.asp)), but the methods should be the same once scaled to the full database available on the cluster.\n",
    "\n",
    "The NOW database is composed of billions of words from online newspapers and magazines from 20 different countries. The data we downloaded comes in different files which can be used together or independently. These files are:\n",
    "\n",
    "1. **now-samples-lexicon.txt**: this is the full dictionnary of the english language, a lexicon. It contains four clolumns, `wID` which is the word id, `word` the actual word, `lemma` which is family of the word (ie: if word is \"walked\", lemma is \"walk\") and `PoS` which is the part of speech.\n",
    "2. **now-samples_sources.txt**: this is the source of every text, in order it contains the text id, the number of words, the date, the country, the website, the url and title of the article.\n",
    "3. **text.txt**: this file has the complete texts of the articles, the first column is the `textID` in the format @@textID, the second column is the full text, complete with html paragraphs and headers. It is important to note that to prevent plagiarism, every 200 words, 10 words are replaced by the string \"@ @ @ @ @ @ @ @ @ @\". Combined words are also split, example \"can't\" is written as \"ca n't\" and punctuation is surrounded by spaces.\n",
    "4. **wordLemPoS.txt**: finally, this file contains the `word`, `lemma` and `PoS` for each word in the texts, one by one, so one could read the texts by reading down the columns. Along with that is the `textID` from where the word is and an `ID (seq)` which we don't know what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need from the NOW corpus for LDA\n",
    "The model will take two inputs, a matrix with all the important words for each text, and a list of all the important words. By important, it is meant the words which will give us good topic modelling. For example, names, locations, simple words like \"but, \"I\" or \"and\" will not give meaningfull results and are quite common in english (so-called stopwords). Other common words present in our database should be removed too. We also should use lemmas instead of words.\n",
    "\n",
    "Therefore, the file `wordLemPoS.txt` (hence referred as wlp) is the most important here as it lists all the lemmas with their `textID` associated. Which means that with it we can lsit all the lemmas, remove those we do not want to make our word list, but also group them by texts to create our text-word matrix.\n",
    "\n",
    "We will also need `now-sample_sources.txt` (hence referred as sources) to link the texts with the information we will deem useful. For example country, date or website.\n",
    "\n",
    "These are thus the two file we will import and process here with the sample data but also those we will use with the data on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wlp processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to extract the useful data from wlp text files. Since they contain all the words of all the articles and the lemmas to replace them with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read the text file\n",
    "wlp_rdd = sc.textFile('sample_data/wordLem_poS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first 3 lines are useless headlines\n",
    "header = wlp_rdd.take(3)\n",
    "\n",
    "#so let's remove those headlines\n",
    "noheaders = wlp_rdd.filter(lambda r: r != header[0])\\\n",
    "            .filter(lambda r: r != header[1])\\\n",
    "            .filter(lambda r: r != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------+-------+\n",
      "|     idseq| lemma| pos|textID|   word|\n",
      "+----------+------+----+------+-------+\n",
      "|1095362496|      |  fo| 11241|@@11241|\n",
      "|1095362497|      |null| 11241|    <p>|\n",
      "|1095362498|   sol| np1| 11241|    Sol|\n",
      "|1095362499|yurick| np1| 11241| Yurick|\n",
      "|1095362500|      |   ,| 11241|      ,|\n",
      "+----------+------+----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we split the elements separated by tabs\n",
    "lines = noheaders.map(lambda r: r.split('\\t'))\n",
    "#identify the columns\n",
    "wlp_schema = lines.map(lambda r: Row(textID=int(r[0]),idseq=int(r[1]),word=r[2],lemma=r[3],pos=r[4]))\n",
    "wlp = spark.createDataFrame(wlp_schema)\n",
    "wlp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| lemma|textID|\n",
      "+------+------+\n",
      "|   sol| 11241|\n",
      "|yurick| 11241|\n",
      "|   the| 11241|\n",
      "|writer| 11241|\n",
      "| whose| 11241|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#keep only useful information\n",
    "wlp = wlp.drop('idseq', 'pos', 'word').filter(wlp['lemma']!='')\n",
    "wlp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can install the stopwords from the module stop-words\n",
    "# pip install stop-words\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_these_words = pd.read_csv('stop-words.txt',delimiter=', ',header=None, engine='python').T\n",
    "stopwords = not_these_words[0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "We are filtering out lemmas based on a stopword list, does that make sense ? Also, numbers like 19 are probably not filtered out since stop-words.txt only contains number from 0 to 9. Maybe consider remove them using the PoS before ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     lemma|count|\n",
      "+----------+-----+\n",
      "|        's| 9878|\n",
      "|         '| 5976|\n",
      "|      year| 4272|\n",
      "|       n't| 3841|\n",
      "|      time| 3169|\n",
      "|    people| 2913|\n",
      "|      take| 2667|\n",
      "|       use| 2244|\n",
      "|      work| 2137|\n",
      "|       day| 1819|\n",
      "|     state| 1713|\n",
      "|   company| 1698|\n",
      "|   comment| 1667|\n",
      "|      need| 1654|\n",
      "|      want| 1579|\n",
      "|      look| 1564|\n",
      "|     world| 1553|\n",
      "|government| 1551|\n",
      "|         -| 1524|\n",
      "|      show| 1480|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter out stopwords ans looking at the frquency of words without them\n",
    "wlp_nostop = wlp.filter(wlp['lemma'].isin(stopwords) == False)\n",
    "wlp_nostop.groupBy('lemma').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Changed wlp to wlp_nostop because otherwise it doesn't make sense. We just want the texts with the filtered words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textID|               lemma|\n",
      "+------+--------------------+\n",
      "| 11241|[yurick, writer, ...|\n",
      "| 11242|[dialect, society...|\n",
      "| 11243|[sublime, croissa...|\n",
      "| 11244|[reflect, quarter...|\n",
      "| 21242|[ars, facebook, c...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouping the selected words by text\n",
    "wlp_bytext=wlp_nostop.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "            .sort('textID')\\\n",
    "            .withColumnRenamed('collect_list(lemma)','lemma')\n",
    "wlp_bytext.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all lemma to extract most and least common\n",
    "This is by Luca, to be able to remove lemmas from the rest of our word lists. This is an alternative to using stop words and using sql (because why not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| lemma|\n",
      "+------+\n",
      "|   sol|\n",
      "|yurick|\n",
      "|   the|\n",
      "|writer|\n",
      "| whose|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_lemmas = wlp.drop('textID')\n",
    "all_lemmas.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_freq = all_lemmas.groupby('lemma').count().sort('count', ascending=False)\n",
    "lemmas_tokeep = lemmas_freq.where('count<7000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlp.registerTempTable('wlp')\n",
    "lemmas_tokeep.registerTempTable('lemma_tokeep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textID|               lemma|\n",
      "+------+--------------------+\n",
      "| 11241|[1970s, some, dec...|\n",
      "| 11242|[online, some, so...|\n",
      "| 11243|[art, some, some,...|\n",
      "| 11244|[..., ..., art, a...|\n",
      "| 21242|[..., online, som...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT wlp.lemma, wlp.textID\n",
    "FROM wlp\n",
    "INNER JOIN lemma_tokeep ON wlp.lemma = lemma_tokeep.lemma\n",
    "order by textID\n",
    "\"\"\"\n",
    "\n",
    "wlp_kept = spark.sql(query)\n",
    "wlp_kept_bytext = wlp_kept.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "                    .sort('textID')\\\n",
    "                    .withColumnRenamed('collect_list(lemma)','lemma')\n",
    "wlp_kept_bytext.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "Contains all the additional informations about each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_rdd = sc.textFile('sample_data/now-samples-sources.txt')\\\n",
    "                .map(lambda r: r.split('\\t'))\n",
    "\n",
    "header = sources_rdd.take(3)\n",
    "sources_rdd = sources_rdd.filter(lambda l: l != header[0])\\\n",
    "                .filter(lambda l: l != header[1])\\\n",
    "                .filter(lambda l: l != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_schema = sources_rdd.map(lambda r: Row(textID=int(r[0]),nwords=int(r[1]),date=r[2],country=r[3],website=r[4],url=r[5],title=r[6],)) \n",
    "sources = spark.createDataFrame(sources_schema)\n",
    "sources = sources.withColumn('date',to_date(sources.date, 'yy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- nwords: long (nullable = true)\n",
      " |-- textID: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|country|      date|nwords|textID|               title|                 url|            website|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|     US|2013-01-06|   397| 11241|Author of The War...|http://kotaku.com...|             Kotaku|\n",
      "|     US|2013-01-06|   757| 11242|That's What They ...|http://michiganra...|     Michigan Radio|\n",
      "|     US|2013-01-06|   755| 11243|Best of New York:...|http://www.nydail...|New York Daily News|\n",
      "|     US|2013-01-06|  1677| 11244|Reflecting on a q...|http://www.oregon...|     OregonLive.com|\n",
      "|     US|2013-01-11|   794| 21242|Ask Ars: Does Fac...|http://arstechnic...|       Ars Technica|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Basically useless, should be removed of you agree.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = sc.textFile('sample_data/text.txt') \\\n",
    "            .filter(lambda r: len(r)>20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw_schema = text_rdd.map(lambda r: Row(text=r)) \n",
    "text_raw = spark.createDataFrame(text_raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|textID|\n",
      "+--------------------+------+\n",
      "|<p> Sol Yurick , ...| 11241|\n",
      "|<h> That 's What ...| 11242|\n",
      "|<h> A sublime cro...| 11243|\n",
      "|<h> Reflecting on...| 11244|\n",
      "|<h> Ask Ars : Doe...| 21242|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_raw = text_raw.withColumn('textID', regexp_extract('text','(\\d+)',1))\n",
    "text = text_raw.rdd.map(lambda r: (re.sub('@@\\d+ ','',r[0]),r[1])).map(lambda r: Row(text=r[0],textID=int(r[1]))).toDF()\n",
    "text.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
