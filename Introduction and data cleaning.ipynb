{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LDA and data cleaning\n",
    "In this notebook, we introduce LDA and what we need for our model. We then proceed to load and clean a sample of the NOW corpus to fulfill our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LDA\n",
    "[Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a statistical model which we will use for topic modelling/discovery. LDA will, given a list of words belonging to a text, output the topics present and their probability. In here, a topic is represented as a probability distribution of words. Thus each text/document will be a distribution over the topics. In short, texts have an associated topic distribution and topics have a word distribution. \n",
    "\n",
    "The image below is the plate notation for LDA, where:\n",
    "* θ<sub>m</sub> is the topic distribution for document m,\n",
    "* φ<sub>k</sub> is the word distribution for topic k,\n",
    "* z<sub>mn</sub> is the topic for the n-th word in document m, and\n",
    "* w<sub>mn</sub> is the specific word.\n",
    "* α is the parameter of the Dirichlet prior on the per-document topic distributions,\n",
    "* β is the parameter of the Dirichlet prior on the per-topic word distribution,\n",
    "\n",
    "![](LDA.png)\n",
    "\n",
    "α and β are the parameters for the model. A big α means that documents are likely to be represented by a high number of topics and vice versa. Same goes for β, a high value meaning that topics are represented by a hign number of words. The number of topics that LDA outputs is dependent on our input and works a bit like clustering. If we allow too many topics we might end up splitting topics uselessly and a too few will make us group them unnecessarily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NOW corpus\n",
    "This notebook shows the cleaning process that will be used for the ADA project. Here, only a sample of the data is used (from [here](https://www.corpusdata.org/now_corpus.asp)), but the methods should be the same once scaled to the full database available on the cluster.\n",
    "\n",
    "The NOW database is composed of billions of words from online newspapers and magazines from 20 different countries. The data we downloaded comes in different files which can be used together or independently. These files are:\n",
    "\n",
    "1. **now-samples-lexicon.txt**: this is the full dictionnary of the english language, a lexicon. It contains four clolumns, `wID` which is the word id, `word` the actual word, `lemma` which is family of the word (ie: if word is \"walked\", lemma is \"walk\") and `PoS` which is the part of speech.\n",
    "2. **now-samples_sources.txt**: this is the source of every text, in order it contains the text id, the number of words, the date, the country, the website, the url and title of the article.\n",
    "3. **text.txt**: this file has the complete texts of the articles, the first column is the `textID` in the format @@textID, the second column is the full text, complete with html paragraphs and headers. It is important to note that to prevent plagiarism, every 200 words, 10 words are replaced by the string \"@ @ @ @ @ @ @ @ @ @\". Combined words are also split, example \"can't\" is written as \"ca n't\" and punctuation is surrounded by spaces.\n",
    "4. **wordLemPoS.txt**: finally, this file contains the `word`, `lemma` and `PoS` for each word in the texts, one by one, so one could read the texts by reading down the columns. Along with that is the `textID` from where the word is and an `ID (seq)` which is a unique indetifier for each word in the database. Each time a word is added this number is incremented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need from the NOW corpus for LDA\n",
    "The model will take two inputs, a matrix with all the important words for each text, and a list of all the important words. By important, it is meant the words which will give us good topic modelling. For example, names, locations, simple words like \"but, \"I\" or \"and\" will not give meaningfull results and are quite common in english (so-called stopwords). Other common words present in our database should be removed too. We also should use lemmas instead of words.\n",
    "\n",
    "Therefore, the file `wordLemPoS.txt` (hence referred as wlp) is the most important here as it lists all the lemmas with their `textID` associated. Which means that with it we can lsit all the lemmas, remove those we do not want to make our word list, but also group them by texts to create our text-word matrix.\n",
    "\n",
    "We will also need `now-sample_sources.txt` (hence referred as sources) to link the texts with the information we will deem useful. For example country, date or website.\n",
    "\n",
    "These are thus the two file we will import and process here with the sample data but also those we will use with the data on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wlp processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to extract the useful data from wlp text files. Since they contain all the words of all the articles and the lemmas to replace them with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read the text file\n",
    "wlp_rdd = sc.textFile('sample_data/wordLem_poS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first 3 lines are useless headlines\n",
    "header = wlp_rdd.take(3)\n",
    "\n",
    "#so let's remove those headlines\n",
    "noheaders = wlp_rdd.filter(lambda r: r != header[0])\\\n",
    "            .filter(lambda r: r != header[1])\\\n",
    "            .filter(lambda r: r != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------+-------+\n",
      "|     idseq| lemma| pos|textID|   word|\n",
      "+----------+------+----+------+-------+\n",
      "|1095362496|      |  fo| 11241|@@11241|\n",
      "|1095362497|      |null| 11241|    <p>|\n",
      "|1095362498|   sol| np1| 11241|    Sol|\n",
      "|1095362499|yurick| np1| 11241| Yurick|\n",
      "|1095362500|      |   ,| 11241|      ,|\n",
      "+----------+------+----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we split the elements separated by tabs\n",
    "lines = noheaders.map(lambda r: r.split('\\t'))\n",
    "\n",
    "#identify the columns\n",
    "wlp_schema = lines.map(lambda r: Row(textID=int(r[0]),idseq=int(r[1]),word=r[2],lemma=r[3],pos=r[4]))\n",
    "wlp = spark.createDataFrame(wlp_schema)\n",
    "wlp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word selection\n",
    "It is very important to select the right words and the right number. The ocncept of \"garbage in garbage out\" has never been more true than with LDA. When we analyse a text we focus on certain words to extract it's meaning and topic. The same is true here since words like if, for, numbers, common names are not that useful.\n",
    "\n",
    "Here, we provide and example of the process we will go through. However this is not really a data cleaning step as it will directly influence our model. It is more of a model preprocessing step. We will surely go through many iterations of this next part for our model to give the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can remove all the words which have a PoS which do not interest us. For example number (`mc`,`mc1`,`m#`) or punctuation (`.`,`'`), etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_remove = ['.',',',\"\\'\",'\\\"','null','mc','mc1','m#']\n",
    "wlp_nopos = wlp.drop('idseq','pos','word').filter(~wlp['pos'].isin(pos_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Drop least frequent pos\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load our list of stopwords, the words that we are not going to use in LDA as they are too common or are common names. We can also remove the rows with no lemmas or those with lemmas that don't make sense or are not common enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5636"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.save('our_stopwords',stopwords)\n",
    "stopwords = np.load('our_stopwords.npy').tolist()\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('our_stopwords',stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|            lemma|count|\n",
      "+-----------------+-----+\n",
      "|         barbizon|    1|\n",
      "|             tant|    1|\n",
      "|          pitcher|    1|\n",
      "|        soundscan|    1|\n",
      "|    city-operated|    1|\n",
      "|             iffy|    1|\n",
      "|        librarian|    1|\n",
      "| battery-operated|    1|\n",
      "|      baby-boomer|    1|\n",
      "|             chor|    1|\n",
      "|recently-released|    1|\n",
      "|            pujol|    1|\n",
      "|     photovoltaic|    1|\n",
      "|       misogynist|    1|\n",
      "|     bankrate.com|    1|\n",
      "|            meloy|    1|\n",
      "|        destitute|    1|\n",
      "| front-and-center|    1|\n",
      "|    pointy-headed|    1|\n",
      "|      militarized|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter out stopwords and looking at the frquency of words without them\n",
    "wlp_nostop = wlp_nopos.filter(~wlp['lemma'].isin(stopwords)) #.filter(~wlp['lemma'].rlike('\\W'))\n",
    "wlp_nostop.groupBy('lemma').count().sort('count', ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we can group the lemmas in their texts to create our text-word matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textID|               lemma|\n",
      "+------+--------------------+\n",
      "| 11241|[yurick, writer, ...|\n",
      "| 11242|[dialect, society...|\n",
      "| 11243|[sublime, croissa...|\n",
      "| 11244|[reflect, quarter...|\n",
      "| 21242|[ars, facebook, c...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouping the selected words by text\n",
    "wlp_bytext = wlp_nostop.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "                .sort('textID')\\\n",
    "                .withColumnRenamed('collect_list(lemma)','lemma')\n",
    "wlp_bytext.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove the most common and least common lemmas. These will be useless since they won't provide enough information for our LDA analysis. This can be done in sql for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  lemma|\n",
      "+-------+\n",
      "| yurick|\n",
      "| writer|\n",
      "|  novel|\n",
      "|warrior|\n",
      "|  adapt|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_lemmas = wlp_nostop.drop('textID')\n",
    "all_lemmas.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we calculate the frequencies and filter out the top\n",
    "lemmas_freq = all_lemmas.groupby('lemma').count().sort('count', ascending=False)\n",
    "lemmas_tokeep = lemmas_freq.where('count<7000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a inner join, we keep only the words which are in both lists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textID|               lemma|\n",
      "+------+--------------------+\n",
      "| 11241|[1970s, decay, fi...|\n",
      "| 11242|[online, happen, ...|\n",
      "| 11243|[airiest, crust, ...|\n",
      "| 11244|[trail, launch, o...|\n",
      "| 21242|[online, launch, ...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perform sql query and inner join\n",
    "wlp_nostop.registerTempTable('wlp_nostop')\n",
    "lemmas_tokeep.registerTempTable('lemma_tokeep')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT wlp_nostop.lemma, wlp_nostop.textID\n",
    "FROM wlp_nostop\n",
    "INNER JOIN lemma_tokeep ON wlp_nostop.lemma = lemma_tokeep.lemma\n",
    "order by textID\n",
    "\"\"\"\n",
    "\n",
    "wlp_kept = spark.sql(query)\n",
    "wlp_kept_bytext = wlp_kept.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "                    .sort('textID')\\\n",
    "                    .withColumnRenamed('collect_list(lemma)','lemma')\n",
    "wlp_kept_bytext.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "Contains all the additional informations about each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_rdd = sc.textFile('sample_data/now-samples-sources.txt')\\\n",
    "                .map(lambda r: r.split('\\t'))\n",
    "\n",
    "header = sources_rdd.take(3)\n",
    "sources_rdd = sources_rdd.filter(lambda l: l != header[0])\\\n",
    "                .filter(lambda l: l != header[1])\\\n",
    "                .filter(lambda l: l != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create schema and change data type for date\n",
    "sources_schema = sources_rdd.map(lambda r: Row(textID=int(r[0]),nwords=int(r[1]),date=r[2],country=r[3],website=r[4],url=r[5],title=r[6],)) \n",
    "sources = spark.createDataFrame(sources_schema)\n",
    "sources = sources.withColumn('date',to_date(sources.date, 'yy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- nwords: long (nullable = true)\n",
      " |-- textID: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|country|      date|nwords|textID|               title|                 url|            website|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|     US|2013-01-06|   397| 11241|Author of The War...|http://kotaku.com...|             Kotaku|\n",
      "|     US|2013-01-06|   757| 11242|That's What They ...|http://michiganra...|     Michigan Radio|\n",
      "|     US|2013-01-06|   755| 11243|Best of New York:...|http://www.nydail...|New York Daily News|\n",
      "|     US|2013-01-06|  1677| 11244|Reflecting on a q...|http://www.oregon...|     OregonLive.com|\n",
      "|     US|2013-01-11|   794| 21242|Ask Ars: Does Fac...|http://arstechnic...|       Ars Technica|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
