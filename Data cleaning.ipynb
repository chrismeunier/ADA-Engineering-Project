{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the NOW corpus\n",
    "This notebook shows the cleaning process that will be used for the ADA project. Here, only a sample of the data is used (from [here](https://www.corpusdata.org/now_corpus.asp)), but the methods should be the same once scaled to the full database.\n",
    "\n",
    "The NOW database is composed of billions of words from online newspapers and magazines from 20 different countries. The data we downloaded comes in different files which can be used together or independently. These files are:\n",
    "\n",
    "1. **now-samples-lexicon.txt**: this is the full dictionnary of the english language, a lexicon. It contains four clolumns, `wID` which is the word id, `word` the actual word, `lemma` which is family of the word (ie: if word is \"walked\", lemma is \"walk\") and `PoS` which is the part of speech.\n",
    "2. **now-samples_sources.txt**: this is the source of every text, in order it contains the text id, the number of words, the date, the country, the website, the url and title of the article.\n",
    "3. **text.txt**: this file has the complete texts of the articles, the first column is the `textID` in the format @@textID, the second column is the full text, complete with html paragraphs and headers. It is important to note that to prevent plagiarism, every 200 words, 10 words are replaced by the string \"@ @ @ @ @ @ @ @ @ @\". Combined words are also split, example \"can't\" is written as \"ca n't\" and punctuation is surrounded by spaces.\n",
    "4. **wordLemPoS.txt**: finally, this file contains the `word`, `lemma` and `PoS` for each word in the texts, one by one, so one could read the texts by reading down the columns. Along with that is the `textID` from where the word is and an `ID (seq)` which we don't know what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install stop-words\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Lemma-PoS processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to extract the useful data from wlp text files. Since they contain all the words of all the articles and the \"lemmas\" to replace them with.\n",
    "This code serves as an example to show how to treat such a file on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read the text file\n",
    "wlp_rdd = sc.textFile('sample_data/wordLem_poS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first 3 lines are useless headlines\n",
    "header = wlp_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's remove those headlines\n",
    "noheaders = wlp_rdd.filter(lambda l: l != header[0])\\\n",
    ".filter(lambda l: l != header[1])\\\n",
    ".filter(lambda l: l != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the elements separated by tabs\n",
    "lines = noheaders.map(lambda l: l.split(\"\\t\"))\n",
    "#identify the columns\n",
    "frame = lines.map(lambda p: Row(textid=int(p[0]),idseq=int(p[1]),word=p[2],lemma=p[3],pos=p[4]))\n",
    "df = spark.createDataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------+-------+\n",
      "|     idseq| lemma| pos|textid|   word|\n",
      "+----------+------+----+------+-------+\n",
      "|1095362496|      |  fo| 11241|@@11241|\n",
      "|1095362497|      |null| 11241|    <p>|\n",
      "|1095362498|   sol| np1| 11241|    Sol|\n",
      "|1095362499|yurick| np1| 11241| Yurick|\n",
      "|1095362500|      |   ,| 11241|      ,|\n",
      "+----------+------+----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only useful information\n",
    "df = df.drop(\"idseq\", \"pos\", \"word\").filter(df[\"lemma\"]!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| lemma|textid|\n",
      "+------+------+\n",
      "|   sol| 11241|\n",
      "|yurick| 11241|\n",
      "|   the| 11241|\n",
      "|writer| 11241|\n",
      "| whose| 11241|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.filter(df[\"lemma\"].isin(stop_words) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|        lemma|count|\n",
      "+-------------+-----+\n",
      "|           's| 9878|\n",
      "|          say| 9221|\n",
      "|         will| 6407|\n",
      "|            '| 5976|\n",
      "|         year| 4272|\n",
      "|          one| 3897|\n",
      "|          can| 3860|\n",
      "|          n't| 3841|\n",
      "|         make| 3448|\n",
      "|         also| 3361|\n",
      "|         time| 3169|\n",
      "|          new| 3122|\n",
      "|          get| 3031|\n",
      "|       people| 2913|\n",
      "|           go| 2859|\n",
      "|         take| 2667|\n",
      "|         like| 2356|\n",
      "|          use| 2244|\n",
      "|        first| 2155|\n",
      "|         work| 2137|\n",
      "|           us| 2134|\n",
      "|         come| 2116|\n",
      "|          see| 2115|\n",
      "|         just| 2082|\n",
      "|          two| 2055|\n",
      "|          now| 1943|\n",
      "|          day| 1819|\n",
      "|         last| 1769|\n",
      "|        state| 1713|\n",
      "|      company| 1698|\n",
      "|      comment| 1667|\n",
      "|         need| 1654|\n",
      "|         know| 1652|\n",
      "|          may| 1611|\n",
      "|         want| 1579|\n",
      "|         look| 1564|\n",
      "|        world| 1553|\n",
      "|   government| 1551|\n",
      "|            -| 1524|\n",
      "|         show| 1480|\n",
      "|         give| 1480|\n",
      "|         many| 1468|\n",
      "|      country| 1465|\n",
      "|         find| 1464|\n",
      "|         even| 1461|\n",
      "|          way| 1431|\n",
      "|        think| 1409|\n",
      "|        right| 1408|\n",
      "|         back| 1390|\n",
      "|         well| 1375|\n",
      "|         city| 1373|\n",
      "|         team| 1349|\n",
      "|         call| 1313|\n",
      "|         tell| 1295|\n",
      "|         help| 1281|\n",
      "|          man| 1273|\n",
      "|         game| 1265|\n",
      "|         high| 1256|\n",
      "|     business| 1237|\n",
      "|         good| 1210|\n",
      "|         play| 1199|\n",
      "|      service| 1196|\n",
      "|       report| 1194|\n",
      "|         part| 1187|\n",
      "|       school| 1183|\n",
      "|         life| 1178|\n",
      "|        group| 1174|\n",
      "|       around| 1170|\n",
      "|       police| 1114|\n",
      "|        child| 1112|\n",
      "|        start| 1112|\n",
      "|       change| 1111|\n",
      "|         home| 1092|\n",
      "|        three| 1088|\n",
      "|        thing| 1084|\n",
      "|         much| 1080|\n",
      "|        still| 1068|\n",
      "|       family| 1066|\n",
      "|         week| 1066|\n",
      "|        month| 1064|\n",
      "|           mr| 1056|\n",
      "|        share| 1054|\n",
      "|        woman| 1046|\n",
      "|          big| 1046|\n",
      "|          add| 1043|\n",
      "|       market| 1029|\n",
      "|        since| 1019|\n",
      "|        place| 1013|\n",
      "|        india| 1008|\n",
      "|         next| 1005|\n",
      "|      million|  985|\n",
      "|          far|  976|\n",
      "|         area|  967|\n",
      "|         long|  960|\n",
      "|        point|  956|\n",
      "|       become|  956|\n",
      "|          per|  953|\n",
      "|          ...|  952|\n",
      "|      provide|  945|\n",
      "|          win|  945|\n",
      "|     national|  932|\n",
      "|          end|  928|\n",
      "|      another|  925|\n",
      "|        issue|  919|\n",
      "|          run|  913|\n",
      "|       number|  908|\n",
      "|        leave|  904|\n",
      "|         news|  901|\n",
      "|       member|  901|\n",
      "|       public|  898|\n",
      "|    community|  896|\n",
      "|        court|  889|\n",
      "|  information|  888|\n",
      "|         best|  871|\n",
      "|          ask|  870|\n",
      "|        great|  865|\n",
      "|         keep|  861|\n",
      "|         case|  860|\n",
      "|         name|  850|\n",
      "|        early|  850|\n",
      "|      project|  848|\n",
      "|         lead|  847|\n",
      "|        bring|  847|\n",
      "|         plan|  845|\n",
      "|          set|  836|\n",
      "|          try|  822|\n",
      "|         post|  821|\n",
      "|      support|  820|\n",
      "|        local|  820|\n",
      "|      however|  815|\n",
      "|        house|  804|\n",
      "|    including|  802|\n",
      "|    president|  798|\n",
      "|       system|  794|\n",
      "|         move|  789|\n",
      "|       follow|  782|\n",
      "|        story|  781|\n",
      "|          pay|  774|\n",
      "|     continue|  768|\n",
      "|        every|  767|\n",
      "|    according|  761|\n",
      "|      without|  756|\n",
      "|          put|  755|\n",
      "|      student|  754|\n",
      "|      include|  754|\n",
      "|       really|  753|\n",
      "|        power|  749|\n",
      "|        party|  748|\n",
      "|         feel|  747|\n",
      "|        price|  745|\n",
      "|       second|  734|\n",
      "|         open|  730|\n",
      "|         mean|  730|\n",
      "|        offer|  730|\n",
      "|         live|  727|\n",
      "|         hold|  725|\n",
      "|   university|  720|\n",
      "|       better|  717|\n",
      "|          top|  712|\n",
      "|          lot|  707|\n",
      "|         four|  701|\n",
      "|     minister|  698|\n",
      "|        today|  693|\n",
      "|        allow|  692|\n",
      "|       center|  688|\n",
      "|        never|  688|\n",
      "|         cost|  683|\n",
      "|        young|  679|\n",
      "|        event|  675|\n",
      "|        media|  670|\n",
      "|        large|  668|\n",
      "|       former|  667|\n",
      "|       create|  663|\n",
      "|         base|  656|\n",
      "|      release|  651|\n",
      "|          job|  648|\n",
      "|  development|  648|\n",
      "|        write|  642|\n",
      "|         film|  641|\n",
      "|       result|  638|\n",
      "|    something|  637|\n",
      "|    different|  635|\n",
      "|       health|  634|\n",
      "|         late|  634|\n",
      "|      receive|  633|\n",
      "|         book|  632|\n",
      "|       season|  629|\n",
      "|         head|  628|\n",
      "|international|  628|\n",
      "|      problem|  624|\n",
      "|         face|  622|\n",
      "|     industry|  621|\n",
      "|       little|  621|\n",
      "|         site|  620|\n",
      "|        money|  618|\n",
      "|     official|  617|\n",
      "|        force|  616|\n",
      "|       record|  610|\n",
      "|        south|  609|\n",
      "|       player|  608|\n",
      "+-------------+-----+\n",
      "only showing top 200 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"lemma\").count().sort(\"count\", ascending=False).show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.groupBy(\"textid\").agg(collect_list(\"lemma\"))\\\n",
    "    .sort(\"textid\")\\\n",
    "    .withColumnRenamed(\"collect_list(lemma)\",\"lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textid|               lemma|\n",
      "+------+--------------------+\n",
      "| 11241|[, , sol, yurick,...|\n",
      "| 11242|[, , that, be, wh...|\n",
      "| 11243|[, , a, sublime, ...|\n",
      "| 11244|[, , reflect, on,...|\n",
      "| 21242|[, , ask, ars, , ...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = sc.textFile('sample_data/text.txt') \\\n",
    "            .filter(lambda r: len(r)>20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw_schema = text_rdd.map(lambda r: Row(text=r)) \n",
    "text_raw = spark.createDataFrame(text_raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|textID|\n",
      "+--------------------+------+\n",
      "|<p> Sol Yurick , ...| 11241|\n",
      "|<h> That 's What ...| 11242|\n",
      "|<h> A sublime cro...| 11243|\n",
      "|<h> Reflecting on...| 11244|\n",
      "|<h> Ask Ars : Doe...| 21242|\n",
      "|<p> NEW YORK -- A...| 21243|\n",
      "|<p> IRELAND 'S Ol...| 31240|\n",
      "|<h> Shakira launc...| 31241|\n",
      "|<p> ENTREPRENEUR ...| 31242|\n",
      "|<p> Syrian women ...| 41240|\n",
      "|<h> Published byS...| 41241|\n",
      "|<h> The Bay Bridg...| 41244|\n",
      "|<h> MPAA Lobbies ...| 51243|\n",
      "|<h> Mum 's fight ...| 61240|\n",
      "|<h> IPPC to inves...| 61242|\n",
      "|<p> North America...| 71240|\n",
      "|<h> James Ferguss...| 71241|\n",
      "|<h> From Richard ...| 71242|\n",
      "|<h> ' Incompatibl...| 71243|\n",
      "|<h> Mary Leakey ,...| 71244|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_raw = text_raw.withColumn('textID', regexp_extract('text','(\\d+)',1))\n",
    "text = text_raw.rdd.map(lambda r: (re.sub('@@\\d+ ','',r[0]),r[1])).map(lambda r: Row(text=r[0],textID=r[1])).toDF()\n",
    "text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
