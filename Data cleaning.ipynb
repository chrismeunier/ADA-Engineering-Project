{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the NOW corpus\n",
    "This notebook shows the cleaning process that will be used for the ADA project. Here, only a sample of the data is used (from [here](https://www.corpusdata.org/now_corpus.asp)), but the methods should be the same once scaled to the full database.\n",
    "\n",
    "The NOW database is composed of billions of words from online newspapers and magazines from 20 different countries. The data we downloaded comes in different files which can be used together or independently. These files are:\n",
    "\n",
    "1. **now-samples-lexicon.txt**: this is the full dictionnary of the english language, a lexicon. It contains four clolumns, `wID` which is the word id, `word` the actual word, `lemma` which is family of the word (ie: if word is walked, lemma is walk) and `PoS` which is the part of speech.\n",
    "2. **now-samples_sources.txt**: this is the source of every text, in order it contains the text id, the number of words, the date, the country, the website, the url and title of the article.\n",
    "3. **text.txt**: this file has the complete texts of the articles, the first column is the `textID` in the format @@textID, the second column is the full text, complete with html paragraphs and headers. It is important to note that to prevent plagiarism, every 200 words, 10 words are replaced by the string \"@ @ @ @ @ @ @ @ @ @\". Combined words are also split, example \"can't\" is written as \"ca n't\" and punctuation is surrounded by spaces.\n",
    "4. **wordLemPoS.txt**: finally, this file contains the `word`, `lemma` and `PoS` for each word in the texts, one by one, so one could read the texts by reading doewn the columns. Along with that is the `textID` from where the word is and an `ID (seq)` which we don't know what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = sc.textFile('sample_data/text.txt') \\\n",
    "            .filter(lambda r: len(r)>20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw_schema = text_rdd.map(lambda r: Row(text=r)) \n",
    "text_raw = spark.createDataFrame(text_raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|textID|\n",
      "+--------------------+------+\n",
      "|<p> Sol Yurick , ...| 11241|\n",
      "|<h> That 's What ...| 11242|\n",
      "|<h> A sublime cro...| 11243|\n",
      "|<h> Reflecting on...| 11244|\n",
      "|<h> Ask Ars : Doe...| 21242|\n",
      "|<p> NEW YORK -- A...| 21243|\n",
      "|<p> IRELAND 'S Ol...| 31240|\n",
      "|<h> Shakira launc...| 31241|\n",
      "|<p> ENTREPRENEUR ...| 31242|\n",
      "|<p> Syrian women ...| 41240|\n",
      "|<h> Published byS...| 41241|\n",
      "|<h> The Bay Bridg...| 41244|\n",
      "|<h> MPAA Lobbies ...| 51243|\n",
      "|<h> Mum 's fight ...| 61240|\n",
      "|<h> IPPC to inves...| 61242|\n",
      "|<p> North America...| 71240|\n",
      "|<h> James Ferguss...| 71241|\n",
      "|<h> From Richard ...| 71242|\n",
      "|<h> ' Incompatibl...| 71243|\n",
      "|<h> Mary Leakey ,...| 71244|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_raw = text_raw.withColumn('textID', regexp_extract('text','(\\d+)',1))\n",
    "text = text_raw.rdd.map(lambda r: (re.sub('@@\\d+ ','',r[0]),r[1])).map(lambda r: Row(text=r[0],textID=r[1])).toDF()\n",
    "text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
