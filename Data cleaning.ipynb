{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the NOW corpus\n",
    "This notebook shows the cleaning process that will be used for the ADA project. Here, only a sample of the data is used (from [here](https://www.corpusdata.org/now_corpus.asp)), but the methods should be the same once scaled to the full database available on the cluster.\n",
    "\n",
    "The NOW database is composed of billions of words from online newspapers and magazines from 20 different countries. The data we downloaded comes in different files which can be used together or independently. These files are:\n",
    "\n",
    "1. **now-samples-lexicon.txt**: this is the full dictionnary of the english language, a lexicon. It contains four clolumns, `wID` which is the word id, `word` the actual word, `lemma` which is family of the word (ie: if word is \"walked\", lemma is \"walk\") and `PoS` which is the part of speech.\n",
    "2. **now-samples_sources.txt**: this is the source of every text, in order it contains the text id, the number of words, the date, the country, the website, the url and title of the article.\n",
    "3. **text.txt**: this file has the complete texts of the articles, the first column is the `textID` in the format @@textID, the second column is the full text, complete with html paragraphs and headers. It is important to note that to prevent plagiarism, every 200 words, 10 words are replaced by the string \"@ @ @ @ @ @ @ @ @ @\". Combined words are also split, example \"can't\" is written as \"ca n't\" and punctuation is surrounded by spaces.\n",
    "4. **wordLemPoS.txt**: finally, this file contains the `word`, `lemma` and `PoS` for each word in the texts, one by one, so one could read the texts by reading down the columns. Along with that is the `textID` from where the word is and an `ID (seq)` which we don't know what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "#from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install stop-words\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_these_words = pd.read_csv('stop-words.txt',delimiter=', ',header=None, engine='python').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = not_these_words[0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Lemma-PoS processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to extract the useful data from wlp text files. Since they contain all the words of all the articles and the \"lemmas\" to replace them with.\n",
    "This code serves as an example to show how to treat such a file on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read the text file\n",
    "wlp_rdd = sc.textFile('sample_data/wordLem_poS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first 3 lines are useless headlines\n",
    "header = wlp_rdd.take(3)\n",
    "\n",
    "#so let's remove those headlines\n",
    "noheaders = wlp_rdd.filter(lambda r: r != header[0])\\\n",
    "            .filter(lambda r: r != header[1])\\\n",
    "            .filter(lambda r: r != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split the elements separated by tabs\n",
    "lines = noheaders.map(lambda r: r.split('\\t'))\n",
    "#identify the columns\n",
    "wlp_schema = lines.map(lambda r: Row(textID=int(r[0]),idseq=int(r[1]),word=r[2],lemma=r[3],pos=r[4]))\n",
    "wlp = spark.createDataFrame(wlp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------+-------+\n",
      "|     idseq| lemma| pos|textID|   word|\n",
      "+----------+------+----+------+-------+\n",
      "|1095362496|      |  fo| 11241|@@11241|\n",
      "|1095362497|      |null| 11241|    <p>|\n",
      "|1095362498|   sol| np1| 11241|    Sol|\n",
      "|1095362499|yurick| np1| 11241| Yurick|\n",
      "|1095362500|      |   ,| 11241|      ,|\n",
      "+----------+------+----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wlp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| lemma|textID|\n",
      "+------+------+\n",
      "|   sol| 11241|\n",
      "|yurick| 11241|\n",
      "|   the| 11241|\n",
      "|writer| 11241|\n",
      "| whose| 11241|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#keep only useful information\n",
    "wlp = wlp.drop('idseq', 'pos', 'word').filter(wlp['lemma']!='')\n",
    "wlp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     lemma|count|\n",
      "+----------+-----+\n",
      "|        's| 9878|\n",
      "|         '| 5976|\n",
      "|      year| 4272|\n",
      "|       n't| 3841|\n",
      "|      time| 3169|\n",
      "|    people| 2913|\n",
      "|      take| 2667|\n",
      "|       use| 2244|\n",
      "|      work| 2137|\n",
      "|       day| 1819|\n",
      "|     state| 1713|\n",
      "|   company| 1698|\n",
      "|   comment| 1667|\n",
      "|      need| 1654|\n",
      "|      want| 1579|\n",
      "|      look| 1564|\n",
      "|     world| 1553|\n",
      "|government| 1551|\n",
      "|         -| 1524|\n",
      "|      show| 1480|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wlp_nostop = wlp.filter(wlp['lemma'].isin(stopwords) == False)\n",
    "wlp_nostop.groupBy('lemma').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textID|               lemma|\n",
      "+------+--------------------+\n",
      "| 11241|[sol, yurick, the...|\n",
      "| 11242|[that, be, what, ...|\n",
      "| 11243|[a, sublime, croi...|\n",
      "| 11244|[reflect, on, a, ...|\n",
      "| 21242|[ask, ars, do, fa...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wlp_bytext=wlp.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "            .sort('textID')\\\n",
    "            .withColumnRenamed('collect_list(lemma)','lemma')\n",
    "wlp_bytext.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all lemma to extract most and least common\n",
    "This is by Luca, to be able to remove them from the rest of our word lists. This is an alternative to using stop words and using sql (because why not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| lemma|\n",
      "+------+\n",
      "|   sol|\n",
      "|yurick|\n",
      "|   the|\n",
      "|writer|\n",
      "| whose|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_lemma = wlp.drop('textID')\n",
    "all_lemma.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_freq = all_lemma.groupby('lemma').count().sort('count', ascending=False)\n",
    "lemma_tokeep = lemma_freq.where('count<7000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|lemma|count|\n",
      "+-----+-----+\n",
      "|  sol|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemma_tokeep.where('lemma==\"sol\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlp.registerTempTable('wlp')\n",
    "lemma_tokeep.registerTempTable('lemma_tokeep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|textID|               lemma|\n",
      "+------+--------------------+\n",
      "| 11241|[1970s, some, dec...|\n",
      "| 11242|[online, some, so...|\n",
      "| 11243|[art, some, some,...|\n",
      "| 11244|[..., ..., art, a...|\n",
      "| 21242|[..., online, som...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT wlp.lemma, wlp.textID\n",
    "FROM wlp\n",
    "INNER JOIN lemma_tokeep ON wlp.lemma = lemma_tokeep.lemma\n",
    "order by textID\n",
    "\"\"\"\n",
    "\n",
    "wlp_kept = spark.sql(query)\n",
    "wlp_kept_bytext = wlp_kept.groupBy('textID').agg(collect_list('lemma'))\\\n",
    "                    .sort('textID')\\\n",
    "                    .withColumnRenamed('collect_list(lemma)','lemma')\n",
    "wlp_kept_bytext.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rdd = sc.textFile('sample_data/text.txt') \\\n",
    "            .filter(lambda r: len(r)>20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw_schema = text_rdd.map(lambda r: Row(text=r)) \n",
    "text_raw = spark.createDataFrame(text_raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|textID|\n",
      "+--------------------+------+\n",
      "|<p> Sol Yurick , ...| 11241|\n",
      "|<h> That 's What ...| 11242|\n",
      "|<h> A sublime cro...| 11243|\n",
      "|<h> Reflecting on...| 11244|\n",
      "|<h> Ask Ars : Doe...| 21242|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_raw = text_raw.withColumn('textID', regexp_extract('text','(\\d+)',1))\n",
    "text = text_raw.rdd.map(lambda r: (re.sub('@@\\d+ ','',r[0]),r[1])).map(lambda r: Row(text=r[0],textID=int(r[1]))).toDF()\n",
    "text.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now-samples-sources.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_rdd = sc.textFile('sample_data/now-samples-sources.txt')\\\n",
    "                .map(lambda r: r.split('\\t'))\n",
    "\n",
    "header = sources_rdd.take(3)\n",
    "sources_rdd = sources_rdd.filter(lambda l: l != header[0])\\\n",
    "                .filter(lambda l: l != header[1])\\\n",
    "                .filter(lambda l: l != header[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_schema = sources_rdd.map(lambda r: Row(textID=int(r[0]),nwords=int(r[1]),date=r[2],country=r[3],website=r[4],url=r[5],title=r[6],)) \n",
    "sources = spark.createDataFrame(sources_schema)\n",
    "sources = sources.withColumn('date',to_date(sources.date, 'yy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- nwords: long (nullable = true)\n",
      " |-- textID: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|country|      date|nwords|textID|               title|                 url|            website|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "|     US|2013-01-06|   397| 11241|Author of The War...|http://kotaku.com...|             Kotaku|\n",
      "|     US|2013-01-06|   757| 11242|That's What They ...|http://michiganra...|     Michigan Radio|\n",
      "|     US|2013-01-06|   755| 11243|Best of New York:...|http://www.nydail...|New York Daily News|\n",
      "|     US|2013-01-06|  1677| 11244|Reflecting on a q...|http://www.oregon...|     OregonLive.com|\n",
      "|     US|2013-01-11|   794| 21242|Ask Ars: Does Fac...|http://arstechnic...|       Ars Technica|\n",
      "+-------+----------+------+------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
